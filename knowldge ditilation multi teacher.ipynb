{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, T5Config\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained T5 model with weights\n",
    "student_model_name = \"google/flan-t5-base\" \n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,  # The rank of the LoRA layers\n",
    "    lora_alpha=32,  # Scaling factor for the LoRA layers\n",
    "    lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer as usual\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "teacher_model_name = \"google/flan-t5-large\"\n",
    "# Set the compute dtype to match the input dtype (float16)\n",
    "\n",
    "# Load the model with 8-bit quantization\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    teacher_model_name, \n",
    "    load_in_4bit=True, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "t0_model_name = \"bigscience/T0_3B\"\n",
    "\n",
    "# Load the T0-3B model with 8-bit quantization\n",
    "t0_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    t0_model_name,\n",
    "    load_in_4bit=True, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "flan_t5_xl_model_name = \"google/flan-t5-xl\"\n",
    "\n",
    "# Load the FLAN-T5-XL model with 8-bit quantization\n",
    "flan_t5_xl_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    flan_t5_xl_model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length=256):\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.instances[idx]\n",
    "        input_text = instance[\"input\"]\n",
    "        output_text = instance[\"output\"]\n",
    "        \n",
    "        # Tokenize input text\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Tokenize output text (for labels)\n",
    "        label_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = input_encoding.input_ids.squeeze()\n",
    "        labels = label_encoding.input_ids.squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [f['input_ids'].to(device) for f in features],  \n",
    "            batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            [f['labels'].to(device) for f in features], \n",
    "            batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        # Generate decoder_input_ids by shifting input_ids to the right\n",
    "        decoder_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.cat([torch.tensor([self.tokenizer.pad_token_id]), f['input_ids'].cpu()[:-1]]) \n",
    "             for f in features],\n",
    "            batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Set the correct start token for decoder_input_ids\n",
    "        decoder_input_ids[:, 0] = self.tokenizer.pad_token_id  # Replace with start token id\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': input_ids,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Prepare Data\n",
    "def prepare_data(data_dir, tasks_dir, first_task,last_task):\n",
    "\n",
    "    def load_task_names(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            task_names = file.readlines()\n",
    "        return [task.strip() for task in task_names]\n",
    "\n",
    "    def load_task_data(task_name):\n",
    "        with open(f\"{tasks_dir}/{task_name}.json\", 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def extract_instances(data):\n",
    "        instances = [{\n",
    "            \"input\": data.get(\"Definition\")[0] + \"\\n\" + instance[\"input\"] + \"\\n\",\n",
    "            \"output\": instance[\"output\"][0]\n",
    "        } for instance in data.get(\"Instances\", [])]\n",
    "        return instances\n",
    "\n",
    "    tasks = load_task_names(data_dir)\n",
    "\n",
    "    tasks = tasks[first_task:last_task+1]\n",
    "\n",
    "    all_instances = []\n",
    "    for task in tasks:\n",
    "        task_data = load_task_data(task)\n",
    "        instances = extract_instances(task_data)\n",
    "        all_instances.extend(instances)\n",
    "\n",
    "    return all_instances\n",
    "\n",
    "# Usage Example:\n",
    "train_data_dir = \"./splits/default/train_tasks.txt\"\n",
    "tasks_dir = \"./tasks/\"\n",
    "first_train_task = 1\n",
    "last_train_task = 15\n",
    "\n",
    "# Prepare instances\n",
    "instances = prepare_data(train_data_dir, tasks_dir, first_train_task,last_train_task )\n",
    "\n",
    "# Create Dataset\n",
    "dataset = InstructionDataset(instances, student_tokenizer)\n",
    "dataloader = dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=CustomDataCollator(tokenizer=student_tokenizer),\n",
    "    shuffle = True\n",
    ")\n",
    "  \n",
    "\n",
    "val_data_dir = \"./splits/default/test_tasks.txt\"\n",
    "first_val_tasks = 1 \n",
    "last_val_task = 1\n",
    "# Prepare validation instances\n",
    "val_instances = prepare_data(val_data_dir, tasks_dir, first_val_tasks, last_val_task)\n",
    "\n",
    "# Create Validation Dataset\n",
    "val_dataset = InstructionDataset(val_instances, student_tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=CustomDataCollator(tokenizer=student_tokenizer),\n",
    "    shuffle = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, num_teachers, hidden_sizes):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.num_teachers = num_teachers\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.query_layer = nn.Linear(sum(hidden_sizes), num_teachers)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Concatenate hidden states from all teachers along the last dimension\n",
    "        concat_hidden_states = torch.cat(hidden_states, dim=-1).contiguous().to(torch.float32)        \n",
    "        # Compute attention scores for each instance in the batch\n",
    "        concat_hidden_states = F.relu(concat_hidden_states)\n",
    "        attention_scores = self.query_layer(concat_hidden_states)  # Output shape: [batch_size, seq_length, num_teachers]\n",
    "        \n",
    "        # Take the mean across the batch size and sequence length to get a scalar attention score for each teacher\n",
    "        attention_scores = attention_scores.mean(dim=[0, 1])  # Output shape: [num_teachers]\n",
    "        \n",
    "        # Normalize to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Output shape: [num_teachers]\n",
    "        \n",
    "        return attention_weights  # Now the shape is [num_teachers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy_based_weights(**kwargs):\n",
    "    flan_teacher_logits = kwargs.get('flan_teacher_logits')\n",
    "    t0_teacher_logits = kwargs.get('t0_teacher_logits')\n",
    "    flan_t5_xl_teacher_logits = kwargs.get('flan_t5_xl_teacher_logits')\n",
    "    temperature = kwargs.get('temperature', 1.0)\n",
    "\n",
    "    flan_entropy = torch.distributions.Categorical(logits=flan_teacher_logits).entropy().mean()\n",
    "    t0_entropy = torch.distributions.Categorical(logits=t0_teacher_logits).entropy().mean()\n",
    "    flan_t5_xl_entropy = torch.distributions.Categorical(logits=flan_t5_xl_teacher_logits).entropy().mean()\n",
    "\n",
    "    entropies = torch.tensor([flan_entropy, t0_entropy, flan_t5_xl_entropy])\n",
    "    inv_entropies = 1 / (entropies + 1e-9)\n",
    "    weights = torch.softmax(inv_entropies / temperature, dim=0)\n",
    "    return weights.tolist()\n",
    "\n",
    "def gradient_based_weights(**kwargs):\n",
    "    student_logits = kwargs.get('student_logits')\n",
    "    flan_teacher_logits = kwargs.get('flan_teacher_logits')\n",
    "    t0_teacher_logits = kwargs.get('t0_teacher_logits')\n",
    "    flan_t5_xl_teacher_logits = kwargs.get('flan_t5_xl_teacher_logits')\n",
    "    temperature = kwargs.get('temperature', 1.0)\n",
    "\n",
    "    grads = []\n",
    "    for teacher_logits in [flan_teacher_logits, t0_teacher_logits, flan_t5_xl_teacher_logits]:\n",
    "        loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), torch.argmax(teacher_logits, dim=-1).view(-1))\n",
    "        grad = torch.autograd.grad(loss, student_logits, retain_graph=True)[0]\n",
    "        grad_norm = grad.norm().item()\n",
    "        grads.append(grad_norm)\n",
    "\n",
    "    grads_tensor = torch.tensor(grads)\n",
    "    inv_grads = 1 / (grads_tensor + 1e-9)\n",
    "    weights = torch.softmax(inv_grads / temperature, dim=0)\n",
    "    return weights.tolist()\n",
    "\n",
    "def mutual_information_weights(**kwargs):\n",
    "    student_logits = kwargs.get('student_logits')\n",
    "    flan_teacher_logits = kwargs.get('flan_teacher_logits')\n",
    "    t0_teacher_logits = kwargs.get('t0_teacher_logits')\n",
    "    flan_t5_xl_teacher_logits = kwargs.get('flan_t5_xl_teacher_logits')\n",
    "    temperature = kwargs.get('temperature', 1.0)\n",
    "\n",
    "    def compute_mi(teacher_logits):\n",
    "        joint_distribution = F.softmax((student_logits + teacher_logits) / 2, dim=-1)\n",
    "        marginal_student = F.softmax(student_logits, dim=-1).mean(dim=0)\n",
    "        marginal_teacher = F.softmax(teacher_logits, dim=-1).mean(dim=0)\n",
    "        \n",
    "        epsilon = 1e-10\n",
    "        mi = (joint_distribution * torch.log(joint_distribution / (marginal_student * marginal_teacher + epsilon) + epsilon)).sum()\n",
    "        return mi.item()\n",
    "    \n",
    "    mi_values = [compute_mi(flan_teacher_logits), compute_mi(t0_teacher_logits), compute_mi(flan_t5_xl_teacher_logits)]\n",
    "    mi_tensor = torch.tensor(mi_values)\n",
    "    mi_normalized = (mi_tensor - mi_tensor.min()) / (mi_tensor.max() - mi_tensor.min() + 1e-9)\n",
    "    weights = torch.softmax(mi_normalized / temperature, dim=0)\n",
    "    return weights.tolist()\n",
    "\n",
    "def cross_entropy_weights(**kwargs):\n",
    "    flan_teacher_logits = kwargs.get('flan_teacher_logits')\n",
    "    t0_teacher_logits = kwargs.get('t0_teacher_logits')\n",
    "    flan_t5_xl_teacher_logits = kwargs.get('flan_t5_xl_teacher_logits')\n",
    "\n",
    "    flan_teacher_preds = kwargs.get('flan_teacher_preds')\n",
    "    t0_teacher_preds = kwargs.get('t0_teacher_preds')\n",
    "    flan_t5_xl_teacher_preds = kwargs.get('flan_t5_xl_teacher_preds')\n",
    "    temperature = kwargs.get('temperature', 1.0)\n",
    "\n",
    "    ce_flan_t0 = F.cross_entropy(t0_teacher_logits.view(-1, t0_teacher_logits.size(-1)), flan_teacher_preds.view(-1), reduction='mean')\n",
    "    ce_flan_flant5xl = F.cross_entropy(flan_t5_xl_teacher_logits.view(-1, flan_t5_xl_teacher_logits.size(-1)), flan_teacher_preds.view(-1), reduction='mean')\n",
    "    ce_t0_flan = F.cross_entropy(flan_teacher_logits.view(-1, flan_teacher_logits.size(-1)), t0_teacher_preds.view(-1), reduction='mean')\n",
    "    ce_t0_flant5xl = F.cross_entropy(flan_t5_xl_teacher_logits.view(-1, flan_t5_xl_teacher_logits.size(-1)), t0_teacher_preds.view(-1), reduction='mean')\n",
    "    ce_flant5xl_flan = F.cross_entropy(flan_teacher_logits.view(-1, flan_teacher_logits.size(-1)), flan_t5_xl_teacher_preds.view(-1), reduction='mean')\n",
    "    ce_flant5xl_t0 = F.cross_entropy(t0_teacher_logits.view(-1, t0_teacher_logits.size(-1)), flan_t5_xl_teacher_preds.view(-1), reduction='mean')\n",
    "\n",
    "    ce_losses = [ce_flan_t0, ce_flan_flant5xl, ce_t0_flan, ce_t0_flant5xl, ce_flant5xl_flan, ce_flant5xl_t0]\n",
    "\n",
    "    inv_ce_losses = [1 / ce_loss.item() if ce_loss.item() > 0 else 1/(1e-9) for ce_loss in ce_losses]\n",
    "\n",
    "    weight_flan = (inv_ce_losses[0] + inv_ce_losses[1])\n",
    "    weight_t0 = (inv_ce_losses[2] + inv_ce_losses[3])\n",
    "    weight_flant5xl = (inv_ce_losses[4] + inv_ce_losses[5])\n",
    "\n",
    "    weights = torch.tensor([weight_flan, weight_t0, weight_flant5xl])\n",
    "    weights / weights.norm(dim=-1, keepdim=True)\n",
    "    weights = torch.softmax(weights / temperature, dim=0)\n",
    "    return weights.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(student_logits, teacher_logits, teacher_preds):\n",
    "    # Ensure the logits are in floating point\n",
    "    student_logits = student_logits.float()\n",
    "    teacher_logits = teacher_logits.float()\n",
    "\n",
    "    ce_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), teacher_preds.view(-1), ignore_index=-100)\n",
    "    kl_loss = F.kl_div(F.log_softmax(student_logits, dim=-1), F.softmax(teacher_logits, dim=-1), reduction='batchmean')\n",
    "    return ce_loss, kl_loss\n",
    "\n",
    "\n",
    "def compute_teacher_outputs(model, input_ids, decoder_input_ids, device):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        logits = outputs.logits.to(device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "    return logits, preds\n",
    "\n",
    "\n",
    "def train_epoch(student_model, optimizer, flan_model, t0_model, flan_t5_xl_model, dataloader, device, alpha, temperature, weight_function=None, attention_module=None, use_hard_weighting=False):\n",
    "    student_model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        flan_teacher_logits, flan_teacher_preds = compute_teacher_outputs(flan_model, input_ids, decoder_input_ids, device)\n",
    "        t0_teacher_logits, t0_teacher_preds = compute_teacher_outputs(t0_model, input_ids, decoder_input_ids, device)\n",
    "        flan_t5_xl_teacher_logits, flan_t5_xl_teacher_preds = compute_teacher_outputs(flan_t5_xl_model, input_ids, decoder_input_ids, device)\n",
    "\n",
    "        student_outputs = student_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        if attention_module:\n",
    "            # Gather teacher hidden states\n",
    "            flan_teacher_hidden_states = flan_model.encoder(input_ids=input_ids)[0].to(torch.float16)\n",
    "            t0_teacher_hidden_states = t0_model.encoder(input_ids=input_ids)[0].to(torch.float16)\n",
    "            flan_t5_xl_teacher_hidden_states = flan_t5_xl_model.encoder(input_ids=input_ids)[0].to(torch.float16)\n",
    "            teacher_hidden_states_list=[\n",
    "                    flan_teacher_hidden_states, \n",
    "                    t0_teacher_hidden_states, \n",
    "                    flan_t5_xl_teacher_hidden_states\n",
    "                ]\n",
    "            # Compute teacher weights using the attention module\n",
    "            teacher_weights = attention_module(\n",
    "                teacher_hidden_states_list\n",
    "            )\n",
    "            weight_flan, weight_t0, weight_flant5xl = teacher_weights[0], teacher_weights[1], teacher_weights[2]\n",
    "            # Ensure weights are of shape [batch_size, 1]\n",
    "        elif weight_function:\n",
    "            # Use the provided weight function to compute weights\n",
    "            weights = weight_function(\n",
    "                student_logits=student_logits, \n",
    "                flan_teacher_logits=flan_teacher_logits, \n",
    "                flan_teacher_preds=flan_teacher_preds, \n",
    "                t0_teacher_logits=t0_teacher_logits,\n",
    "                t0_teacher_preds=t0_teacher_preds, \n",
    "                flan_t5_xl_teacher_logits=flan_t5_xl_teacher_logits, \n",
    "                flan_t5_xl_teacher_preds=flan_t5_xl_teacher_preds, \n",
    "                temperature=temperature\n",
    "            )\n",
    "            # Convert the weights to tensors\n",
    "            weights = torch.tensor(weights).to(device)\n",
    "            weight_flan, weight_t0, weight_flant5xl = weights[0], weights[1], weights[2]\n",
    "\n",
    "        else:\n",
    "            # Default to equal weighting if no attention module or weight function is provided\n",
    "            weight_flan = weight_t0 = weight_flant5xl = torch.tensor(1.0 / 3.0).to(device)\n",
    "\n",
    "        if use_hard_weighting:\n",
    "            # Convert soft weights to hard weights\n",
    "            max_weight = max(weight_flan, weight_t0, weight_flant5xl)\n",
    "            weight_flan = torch.tensor(1.0 if weight_flan == max_weight else 0.0).to(device)\n",
    "            weight_t0 = torch.tensor(1.0 if weight_t0 == max_weight else 0.0).to(device)\n",
    "            weight_flant5xl = torch.tensor(1.0 if weight_flant5xl == max_weight else 0.0).to(device)\n",
    "\n",
    "        ce_loss_flan, kl_loss_flan = compute_losses(student_logits, flan_teacher_logits, flan_teacher_preds)\n",
    "        ce_loss_t0, kl_loss_t0 = compute_losses(student_logits, t0_teacher_logits, t0_teacher_preds)\n",
    "        ce_loss_flan_t5_xl, kl_loss_flan_t5_xl = compute_losses(student_logits, flan_t5_xl_teacher_logits, flan_t5_xl_teacher_preds)\n",
    "\n",
    "        ce_loss = (weight_flan * ce_loss_flan + weight_t0 * ce_loss_t0 + weight_flant5xl * ce_loss_flan_t5_xl).mean()\n",
    "\n",
    "        kl_loss = (weight_flan * kl_loss_flan + weight_t0 * kl_loss_t0 + weight_flant5xl * kl_loss_flan_t5_xl).mean()\n",
    "        loss = alpha * ce_loss + (1 - alpha) * kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(\n",
    "            f\"Batch {i+1}/{len(dataloader)} | \"\n",
    "            f\"Total Loss: {total_loss/(i+1):.4f} | \"\n",
    "            f\"Weights -> FLAN: {weight_flan.mean().item():.2f}, T0: {weight_t0.mean().item():.2f}, FLAN-XL: {weight_flant5xl.mean().item():.2f}\"\n",
    "        )\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate_student_model(student_model, val_dataloader, device):\n",
    "    student_model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=-100) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            input_ids = val_batch['input_ids'].to(device)\n",
    "            decoder_input_ids = val_batch['decoder_input_ids'].to(device)\n",
    "            labels = val_batch['labels'].to(device)\n",
    "\n",
    "            student_outputs = student_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, labels=labels)\n",
    "            student_logits = student_outputs.logits\n",
    "\n",
    "            # Compute loss between student model outputs and true labels\n",
    "            ce_loss = loss_function(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
    "            val_loss += ce_loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    return avg_val_loss\n",
    "\n",
    "def train_student_model(student_model, flan_model, flan_t5_xl_model, t0_model, dataloader, val_dataloader, weight_function=None, attention_module=None, use_hard_weighting=False, alpha=0.8, num_epochs=1, lr=5e-5, weight_decay=0.01, temperature=0.5, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    student_model.to(device)\n",
    "    optimizer = Adam(student_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    flan_model.eval()\n",
    "    flan_t5_xl_model.eval()\n",
    "    t0_model.eval()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_loss = train_epoch(student_model, optimizer, flan_model, t0_model, flan_t5_xl_model, dataloader, device, alpha, temperature, weight_function, attention_module, use_hard_weighting)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        avg_val_loss = validate_student_model(student_model, val_dataloader, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_functions = [\n",
    "    entropy_based_weights,\n",
    "    gradient_based_weights,\n",
    "    mutual_information_weights,\n",
    "    cross_entropy_weights\n",
    "]\n",
    "\n",
    "weighting_function_names = [\n",
    "    \"entropy_based\",\n",
    "    \"gradient_based\",\n",
    "    \"mutual_information\",\n",
    "    \"cross_entropy\"\n",
    "]\n",
    "\n",
    "# Iterate through each weighting function\n",
    "for i, weight_function in enumerate(weighting_functions):\n",
    "    for use_hard_weighting in [False, True]:\n",
    "        # Initialize the student model and apply LoRA\n",
    "        student_model_name = \"google/flan-t5-base\"  \n",
    "        student_model = AutoModelForSeq2SeqLM.from_pretrained(student_model_name)\n",
    "        student_model = get_peft_model(student_model, lora_config)\n",
    "        \n",
    "        student_model.to(device)\n",
    "\n",
    "        config_name = f\"{weighting_function_names[i]}_{'hard' if use_hard_weighting else 'soft'}_weighting\"\n",
    "\n",
    "        print(f\"Training with {config_name}\")\n",
    "\n",
    "        train_student_model(\n",
    "            student_model=student_model,\n",
    "            flan_model=flan_model,\n",
    "            flan_t5_xl_model=flan_t5_xl_model,\n",
    "            t0_model=t0_model,\n",
    "            dataloader=dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            weight_function=weight_function,\n",
    "            attention_module=None, \n",
    "            use_hard_weighting=use_hard_weighting,\n",
    "            device=device  \n",
    "        )\n",
    "\n",
    "        # Save the trained model\n",
    "        student_model_save_path = f\"student_model_{config_name}\"\n",
    "        student_model.save_pretrained(student_model_save_path)\n",
    "\n",
    "        del student_model\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_teachers = 3\n",
    "hidden_sizes = [1024, 2048, 2048]  \n",
    "attention_module = AttentionMechanism(num_teachers=num_teachers, hidden_sizes=hidden_sizes)\n",
    "\n",
    "student_model_name = \"google/flan-t5-base\"  \n",
    "student_model = AutoModelForSeq2SeqLM.from_pretrained(student_model_name)\n",
    "student_model = get_peft_model(student_model, lora_config)\n",
    "\n",
    "student_model.to(device)\n",
    "\n",
    "config_name = \"attention_based_weighting\"\n",
    "print(f\"Training with {config_name}\")\n",
    "\n",
    "# Train the student model with attention-based weighting\n",
    "train_student_model(\n",
    "    student_model=student_model,\n",
    "    flan_model=flan_model,\n",
    "    flan_t5_xl_model=flan_t5_xl_model,\n",
    "    t0_model=t0_model,\n",
    "    dataloader=dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    weight_function=None,  # Set this to None when using attention\n",
    "    attention_module=attention_module,  # Use the attention module\n",
    "    use_hard_weighting=False,\n",
    "    device=device  # Ensure model and data are on the correct device\n",
    ")\n",
    "\n",
    "student_model_save_path = f\"student_model_{config_name}\"\n",
    "student_model.save_pretrained(student_model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_name = \"google/flan-t5-base\"  # Adjust the model name if necessary\n",
    "student_model = AutoModelForSeq2SeqLM.from_pretrained(student_model_name)\n",
    "student_model = get_peft_model(student_model, lora_config)\n",
    "\n",
    "student_model.to(device)\n",
    "\n",
    "config_name = \"equal_weighting\"\n",
    "print(f\"Training with {config_name}\")\n",
    "\n",
    "train_student_model(\n",
    "    student_model=student_model,\n",
    "    flan_model=flan_model,\n",
    "    flan_t5_xl_model=flan_t5_xl_model,\n",
    "    t0_model=t0_model,\n",
    "    dataloader=dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    weight_function=None,  \n",
    "    attention_module=None,  \n",
    "    use_hard_weighting=False,\n",
    "    device=device  \n",
    ")\n",
    "\n",
    "student_model_save_path = f\"student_model_{config_name}\"\n",
    "student_model.save_pretrained(student_model_save_path)\n",
    "student_model.save_pretrained(f\"lora_adapter_{config_name}\")\n",
    "\n",
    "del student_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = \"student_model_entropy_based_soft_weighting\"  # Path where the model was saved\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "loaded_model = PeftModel.from_pretrained(base_model, \"lora_adapter\")\n",
    "\n",
    "# Move the model to the device\n",
    "loaded_model.to(device)\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(student_model_name)\n",
    "original_model.to(device)\n",
    "\n",
    "1+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(student_model, text, max_new_tokens=30):\n",
    "    inputs = student_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = student_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        output_scores=True,  \n",
    "        return_dict_in_generate=True  \n",
    "    )\n",
    "    \n",
    "    generated_text = student_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "    \n",
    "    logits = outputs.scores[-2]  \n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the top 5 tokens and their probabilities\n",
    "    top5_probs, top5_tokens = torch.topk(probs, 5, dim=-1)\n",
    "    \n",
    "    # Decode and print the top 5 tokens with their probabilities\n",
    "    print(\"Top 5 most likely tokens and their probabilities:\")\n",
    "    for i in range(5):\n",
    "        token = student_tokenizer.decode(top5_tokens[0, i].item())\n",
    "        probability = top5_probs[0, i].item()\n",
    "        print(f\"Token: '{token}', Probability: {probability:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = \"You are given a question on high school mathematics. You are also given 4 answer options (associated with \\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\"), out of which only one is correct. You need to answer the question by selecting the correct option. You should only answer with the choice letter, not the whole answer.\"\n",
    "input_text = \"What is the greatest common factor of 252 and 96?\\n(A)6 (B)24 (C)5 (D)12\"\n",
    "prompt = f\"{definition}\\n{input_text}\\n\"\n",
    "print(\"\\nflan model\\n\")\n",
    "generate_text(flan_model, prompt)\n",
    "print(\"\\nxl model\\n\")\n",
    "generate_text(flan_t5_xl_model, prompt)\n",
    "print(\"\\nt0 model\\n\")\n",
    "generate_text(t0_model, prompt)\n",
    "print(\"Before training\\n\")\n",
    "generate_text(original_model, prompt)\n",
    "print(\"after  training\\n\")\n",
    "generate_text(loaded_model, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_paths = {\n",
    "    'cross_entropy_soft': \"student_model_cross_entropy_soft_weighting\",\n",
    "    'cross_entropy_hard': \"student_model_cross_entropy_hard_weighting\",\n",
    "    'entropy_based_soft': \"student_model_entropy_based_soft_weighting\",\n",
    "    'entropy_based_hard': \"student_model_entropy_based_hard_weighting\",\n",
    "    'mutual_information_soft': \"student_model_mutual_information_soft_weighting\",\n",
    "    'mutual_information_hard': \"student_model_mutual_information_hard_weighting\",\n",
    "    'gradient_based_soft': \"student_model_gradient_based_soft_weighting\",\n",
    "    'gradient_based_hard': \"student_model_gradient_based_hard_weighting\",\n",
    "    'attention_based_soft': \"student_model_attention_based_weighting_soft\",\n",
    "    'equal_weighting': \"student_model_equal_weighting\"\n",
    "}\n",
    "\n",
    "# Add the base flan-t5 model to the list for evaluation\n",
    "student_model_paths['flan_t5_base'] = \"google/flan-t5-base\"\n",
    "\n",
    "# Define the path to your test data directory\n",
    "test_data_dir = \"./splits/default/test_tasks.txt\"  # Adjust this path as needed\n",
    "tasks_dir = \"./tasks/\"  # Ensure this points to the correct tasks directory\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Prepare the data loader\n",
    "def prepare_dataloader_for_task(test_data_dir, tasks_dir, task_num, tokenizer, max_instances=100):\n",
    "    test_instances = prepare_data(test_data_dir, tasks_dir, task_num, task_num)\n",
    "    test_instances = test_instances[:max_instances]  # Take only the first 100 instances\n",
    "    if len(test_instances) < 600:  # Check if task has fewer than 600 instances\n",
    "        test_dataset = InstructionDataset(test_instances, tokenizer)\n",
    "        test_dataloader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return test_dataloader\n",
    "    return None\n",
    "\n",
    "# Evaluation function to test the student models\n",
    "def evaluate_student_model_with_rouge(model, dataloader, tokenizer, device):\n",
    "    results = []\n",
    "    rouge_scores = []\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating Model\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        references = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids)\n",
    "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            decoded_references = tokenizer.batch_decode(references, skip_special_tokens=True)\n",
    "            results.extend(decoded_preds)\n",
    "\n",
    "            # Calculate ROUGE scores for each prediction\n",
    "            for pred, ref in zip(decoded_preds, decoded_references):\n",
    "                score = scorer.score(ref, pred)\n",
    "                rouge_scores.append(score)\n",
    "\n",
    "    return results, rouge_scores\n",
    "\n",
    "# Define the path to your output file\n",
    "output_file_path = \"average_rouge_scores.txt\"\n",
    "# Iterate over tasks and models, loading one model at a time\n",
    "with open(output_file_path, \"a\") as f:  # Open in append mode to add results to the file\n",
    "    for task_num in range(1,14):\n",
    "        test_dataloader = prepare_dataloader_for_task(test_data_dir, tasks_dir, task_num, tokenizer, max_instances=110)\n",
    "        if test_dataloader:\n",
    "            for model_key, model_path in student_model_paths.items():\n",
    "                if model_key == 'flan_t5_base':\n",
    "                    # Load the base model without LoRA adapter\n",
    "                    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "                else:\n",
    "                    # Load the base FLAN-T5 model first\n",
    "                    base_model = AutoModelForSeq2SeqLM.from_pretrained(student_model_paths['flan_t5_base'])\n",
    "                    # Load the LoRA adapter and combine it with the base model\n",
    "                    model = PeftModel.from_pretrained(base_model, model_path).to(device)\n",
    "                \n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                # Evaluate the model\n",
    "                _, student_rouge_scores = evaluate_student_model_with_rouge(model, test_dataloader, tokenizer, device)\n",
    "\n",
    "                # Calculate average ROUGE scores\n",
    "                avg_score = {\n",
    "                    'rougeL': sum(score['rougeL'].fmeasure for score in student_rouge_scores) / len(student_rouge_scores),\n",
    "                }\n",
    "\n",
    "                # Write the average ROUGE scores to the output file\n",
    "                f.write(f\"Task: {task_num}\\n\")\n",
    "                f.write(f\"  Model: {model_key}\\n\")\n",
    "                f.write(f\"    ROUGE-L: {avg_score['rougeL']:.4f}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                # Clear the model from memory\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Average ROUGE scores to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
