{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from transformers import BertTokenizer, RobertaTokenizer, XLNetTokenizer, AdamW\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer, XLNetForSequenceClassification, XLNetTokenizer, BertConfig, RobertaConfig, XLNetConfig, AdamW\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        positions = torch.arange(0, input_ids.size(1)).unsqueeze(0).repeat(input_ids.size(0), 1).to(input_ids.device)\n",
    "        x = self.embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(self.transformer_encoder(x.transpose(0, 1), src_key_padding_mask=~attention_mask.bool()).transpose(0, 1))\n",
    "        logits = self.classifier(x[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "\n",
    "def preprocess_data(data):\n",
    "    texts = data['text']\n",
    "    labels = data['label']\n",
    "    return texts, labels\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        original_text = self.texts[idx]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label,\n",
    "            'original_text': original_text\n",
    "        }\n",
    "    \n",
    "\n",
    "def combined_loss(logits, soft_logits_list, labels, alpha=0.5):\n",
    "    kl_loss = 0\n",
    "    \n",
    "    for soft_logits in soft_logits_list:\n",
    "        kl_loss += nn.functional.kl_div(\n",
    "            torch.log_softmax(logits, dim=-1), \n",
    "            torch.softmax(soft_logits, dim=-1), \n",
    "                reduction='batchmean'\n",
    "            )\n",
    "    \n",
    "    kl_loss /= (len(soft_logits_list))\n",
    "\n",
    "    ce_loss = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "    return alpha * kl_loss + (1-alpha) * ce_loss\n",
    "    \n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def load_model(model_class, tokenizer_class, model_path, config_class, pretrained_model_name):\n",
    "    config = config_class.from_pretrained(model_path)\n",
    "    model = model_class.from_pretrained(model_path, config=config)\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def train_student(student, dataloader, teacher_model_tokenizer, valid_loader,model_name, epochs=6, alpha=0.5):\n",
    "    student.train()\n",
    "    student.to(device)\n",
    "    optimizer = AdamW(student.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            batch_size = input_ids.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = student(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Get the soft logits from the teacher models dynamically\n",
    "            soft_logits_lists = []\n",
    "            for teacher in teacher_model_tokenizer:\n",
    "                teacher_model = teacher[\"model\"]\n",
    "                tokenizer = teacher[\"tokenizer\"]\n",
    "                teacher_model.eval()\n",
    "                teacher_model.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Tokenize the entire batch using the teacher's tokenizer\n",
    "                    encoded_batch = tokenizer(batch['original_text'], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "                    teacher_input_ids = encoded_batch['input_ids']\n",
    "                    teacher_attention_mask = encoded_batch['attention_mask']\n",
    "                    soft_logits = teacher_model(teacher_input_ids, attention_mask=teacher_attention_mask).logits\n",
    "                    soft_logits_lists.append(soft_logits)\n",
    "\n",
    "            # Calculate the mean of the soft logits from all teacher models\n",
    "            mean_soft_logits = torch.mean(torch.stack(soft_logits_lists), dim=0)\n",
    "            \n",
    "            # Compute loss with the most common labels\n",
    "            most_common_labels = []\n",
    "            for i in range(batch_size):\n",
    "                predictions = [torch.argmax(mean_soft_logits[i]).item()]\n",
    "                most_common_label = max(set(predictions), key=predictions.count)\n",
    "                most_common_labels.append(most_common_label)\n",
    "            most_common_labels = torch.tensor(most_common_labels).to(device)\n",
    "\n",
    "            total_batch_loss = combined_loss(logits, mean_soft_logits, most_common_labels, alpha)\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += total_batch_loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}')\n",
    "        avg_loss, accuracy = evaluate_model(student, valid_loader)\n",
    "        print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    # Save the model after training\n",
    "    torch.save(student.state_dict(), f\"{model_name}.pt\")\n",
    "    print(f\"Model saved as {model_name}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_texts, train_labels = preprocess_data(dataset['train'])\n",
    "test_texts, test_labels = preprocess_data(dataset['test'])\n",
    "\n",
    "# Define paths\n",
    "bert_model_path = './bert_model/'\n",
    "roberta_model_path = './roberta_model'\n",
    "xlnet_model_path = './xlnet_model'\n",
    "\n",
    "# Load models and tokenizers\n",
    "bert_model, bert_tokenizer = load_model(BertForSequenceClassification, BertTokenizer, bert_model_path, BertConfig, \"bert-base-uncased\")\n",
    "roberta_model, roberta_tokenizer = load_model(RobertaForSequenceClassification, RobertaTokenizer, roberta_model_path, RobertaConfig, \"roberta-base\")\n",
    "xlnet_model, xlnet_tokenizer = load_model(XLNetForSequenceClassification, XLNetTokenizer, xlnet_model_path, XLNetConfig, \"xlnet-base-cased\")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset_bert = SentimentDataset(train_texts, train_labels, bert_tokenizer)\n",
    "train_dataset_roberta = SentimentDataset(train_texts, train_labels, roberta_tokenizer)\n",
    "train_dataset_xlnet = SentimentDataset(train_texts, train_labels, xlnet_tokenizer)\n",
    "\n",
    "\n",
    "test_dataset_bert = SentimentDataset(test_texts, test_labels, bert_tokenizer)\n",
    "test_dataset_roberta = SentimentDataset(test_texts, test_labels, roberta_tokenizer)\n",
    "test_dataset_xlnet = SentimentDataset(test_texts, test_labels, xlnet_tokenizer)\n",
    "\n",
    "# Create a single shuffled order of indices\n",
    "indices = list(range(len(train_dataset_bert)))\n",
    "random.shuffle(indices)\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, sampler=sampler)\n",
    "train_loader_roberta = DataLoader(train_dataset_roberta, batch_size=batch_size, sampler=sampler)\n",
    "train_loader_xlnet = DataLoader(train_dataset_xlnet, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_loader_roberta = DataLoader(test_dataset_roberta, batch_size=batch_size, shuffle=True)\n",
    "test_loader_xlnet = DataLoader(test_dataset_xlnet, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Move models to device\n",
    "roberta_model.to(device)\n",
    "xlnet_model.to(device)\n",
    "bert_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define student models\n",
    "combined_student_model = TransformerModel(vocab_size=len(bert_tokenizer), embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers, hidden_dim=hidden_dim, dropout=dropout)\n",
    "student_model_bert = TransformerModel(vocab_size=len(bert_tokenizer), embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers, hidden_dim=hidden_dim, dropout=dropout)\n",
    "student_model_roberta = TransformerModel(vocab_size=len(roberta_tokenizer), embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers, hidden_dim=hidden_dim, dropout=dropout)\n",
    "student_model_xlnet = TransformerModel(vocab_size=len(xlnet_tokenizer), embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers, hidden_dim=hidden_dim, dropout=dropout)\n",
    "\n",
    "# Move student models to device\n",
    "combined_student_model.to(device)\n",
    "student_model_bert.to(device)\n",
    "student_model_roberta.to(device)\n",
    "student_model_xlnet.to(device)\n",
    "\n",
    "\n",
    "student_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, sampler=sampler)\n",
    "test_dataset_student = SentimentDataset(test_texts, test_labels, bert_tokenizer)\n",
    "test_loader_student = DataLoader(test_dataset_student, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined student Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\halpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Loss: 0.6803152970755169\n",
      "Validation Loss: 0.6070, Accuracy: 0.7078\n",
      "Epoch 2/6, Loss: 0.6427579588487372\n",
      "Validation Loss: 0.5586, Accuracy: 0.7486\n",
      "Epoch 3/6, Loss: 0.6310390128360218\n",
      "Validation Loss: 0.5392, Accuracy: 0.7712\n",
      "Epoch 4/6, Loss: 0.6196214401485519\n",
      "Validation Loss: 0.5085, Accuracy: 0.7855\n",
      "Epoch 5/6, Loss: 0.612385183763443\n",
      "Validation Loss: 0.4951, Accuracy: 0.8043\n",
      "Epoch 6/6, Loss: 0.603880204608329\n",
      "Validation Loss: 0.4981, Accuracy: 0.8061\n",
      "Model saved as combined_student_Model.pt\n",
      "bert student Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\halpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Loss: 0.6823795293663376\n",
      "Validation Loss: 0.6087, Accuracy: 0.7097\n",
      "Epoch 2/6, Loss: 0.6457242638692593\n",
      "Validation Loss: 0.5526, Accuracy: 0.7565\n",
      "Epoch 3/6, Loss: 0.628658787092946\n",
      "Validation Loss: 0.5292, Accuracy: 0.7803\n",
      "Epoch 4/6, Loss: 0.6184460613030466\n",
      "Validation Loss: 0.5044, Accuracy: 0.7947\n",
      "Epoch 5/6, Loss: 0.6101442033750311\n",
      "Validation Loss: 0.4872, Accuracy: 0.8147\n",
      "Epoch 6/6, Loss: 0.6026526764654915\n",
      "Validation Loss: 0.4942, Accuracy: 0.8007\n",
      "Model saved as bert_student_Model.pt\n",
      "roberta student Model:\n",
      "Epoch 1/6, Loss: 0.6867016060796214\n",
      "Validation Loss: 0.6417, Accuracy: 0.6198\n",
      "Epoch 2/6, Loss: 0.6511006815572313\n",
      "Validation Loss: 0.5620, Accuracy: 0.7356\n",
      "Epoch 3/6, Loss: 0.6343808185573732\n",
      "Validation Loss: 0.5322, Accuracy: 0.7728\n",
      "Epoch 4/6, Loss: 0.6228217849613991\n",
      "Validation Loss: 0.5174, Accuracy: 0.7926\n",
      "Epoch 5/6, Loss: 0.6146679427176809\n",
      "Validation Loss: 0.4958, Accuracy: 0.8073\n",
      "Epoch 6/6, Loss: 0.608493223605214\n",
      "Validation Loss: 0.4887, Accuracy: 0.8093\n",
      "Model saved as roberta_student_Model.pt\n",
      "xlnet student Model:\n",
      "Epoch 1/6, Loss: 0.6854152246232378\n",
      "Validation Loss: 0.6122, Accuracy: 0.6704\n",
      "Epoch 2/6, Loss: 0.6530224668125426\n",
      "Validation Loss: 0.5703, Accuracy: 0.7373\n",
      "Epoch 3/6, Loss: 0.6376762272681629\n",
      "Validation Loss: 0.5296, Accuracy: 0.7747\n",
      "Epoch 4/6, Loss: 0.6252713085593737\n",
      "Validation Loss: 0.5187, Accuracy: 0.7817\n",
      "Epoch 5/6, Loss: 0.6179716255103481\n",
      "Validation Loss: 0.4940, Accuracy: 0.8041\n",
      "Epoch 6/6, Loss: 0.6122511167291335\n",
      "Validation Loss: 0.4921, Accuracy: 0.8107\n",
      "Model saved as xlnet_student_Model.pt\n"
     ]
    }
   ],
   "source": [
    "teacher_model_tokeniser = [ \n",
    "    {\"model\":bert_model ,\"tokenizer\" : bert_tokenizer},\n",
    "    {\"model\":roberta_model, \"tokenizer\" : roberta_tokenizer},\n",
    "    {\"model\":xlnet_model ,\"tokenizer\" : xlnet_tokenizer}\n",
    "    ]\n",
    "\n",
    "\n",
    "print(f'combined student Model:')\n",
    "train_student(combined_student_model, student_dataloader, teacher_model_tokeniser,  test_loader_student,\"combined_student_Model\" , epochs=epochs)\n",
    "\n",
    "\n",
    "print(f'bert student Model:')\n",
    "train_student(student_model_bert, student_dataloader, [{\"model\": bert_model, \"tokenizer\": bert_tokenizer}], test_loader_student, \"bert_student_Model\", epochs=epochs)\n",
    "\n",
    "print(f'roberta student Model:')\n",
    "train_student(student_model_roberta, student_dataloader, [{\"model\": roberta_model, \"tokenizer\": roberta_tokenizer}], test_loader_student,\"roberta_student_Model\", epochs=epochs)\n",
    "\n",
    "print(f'xlnet student Model:')\n",
    "train_student(student_model_xlnet, student_dataloader, [{\"model\": xlnet_model, \"tokenizer\": xlnet_tokenizer}], test_loader_student, \"xlnet_student_Model\",  epochs=epochs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
