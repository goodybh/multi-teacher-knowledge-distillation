{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_info():\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        # Parse the result\n",
    "        lines = result.strip().split('\\n')\n",
    "        info = [line.split(', ') for line in lines]\n",
    "        for idx, (name, total, used, free) in enumerate(info):\n",
    "            print(f\"GPU {idx}: {name}\")\n",
    "            print(f\"  Total memory: {int(total) / 1024:.2f} GB\")\n",
    "            print(f\"  Used memory: {int(used) / 1024:.2f} GB\")\n",
    "            print(f\"  Free memory: {int(free) / 1024:.2f} GB\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "get_gpu_memory_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding = False)\n",
    "\n",
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, instances, tokenizer):\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.instances[idx]\n",
    "        input_text = instance[\"input\"]\n",
    "        output_text = instance[\"output\"]\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        output_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = output_encoding.input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding.input_ids.squeeze(),\n",
    "            \"attention_mask\": input_encoding.attention_mask.squeeze(),\n",
    "            \"labels\": labels.squeeze(),\n",
    "            \"original_text\": input_text\n",
    "        }\n",
    "\n",
    "def prepare_data(data_dir, tasks_dir, number_of_tasks):\n",
    "    def load_task_names(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            task_names = file.readlines()\n",
    "        return [task.strip() for task in task_names]\n",
    "\n",
    "    def load_task_data(task_name):\n",
    "        with open(f\"{tasks_dir}/{task_name}.json\", 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def extract_instances(data):\n",
    "        instances = [{\n",
    "            \"input\": data.get(\"Definition\")[0]+ \"\\n\" + instance[\"input\"] + \"\\n\",\n",
    "            \"output\": instance[\"output\"][0]\n",
    "        } for instance in data.get(\"Instances\", [])]\n",
    "        return instances\n",
    "\n",
    "    tasks = load_task_names(data_dir)\n",
    "\n",
    "    all_instances = []\n",
    "    for task in tasks[:number_of_tasks]:\n",
    "        task_data = load_task_data(task)\n",
    "        instances = extract_instances(task_data)\n",
    "        all_instances.extend(instances)\n",
    "\n",
    "    return all_instances\n",
    "\n",
    "train_data_dir = \"./splits/default/train_tasks.txt\"\n",
    "test_data_dir = \"./splits/default/test_tasks.txt\"\n",
    "tasks_dir = \"./tasks/\"\n",
    "instances = prepare_data(train_data_dir, tasks_dir, 3)\n",
    "test_instances = prepare_data(test_data_dir, tasks_dir, 1)\n",
    "# Create the dataset and dataloader\n",
    "dataset = InstructionDataset(instances, tokenizer)\n",
    "\n",
    "test_dataset = InstructionDataset(test_instances, tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Dynamically pad the sequences\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, dropout=0.1):\n",
    "        super(DecoderOnlyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        positions = torch.arange(0, input_ids.size(1), device=input_ids.device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
    "        x = self.embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        causal_mask = causal_mask.float().masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        memory = torch.zeros((0, x.size(1), x.size(2)), device=x.device)  \n",
    "        x = self.transformer_decoder(x, memory, tgt_mask=causal_mask).transpose(0, 1)\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, input_ids, max_length):\n",
    "        generated_ids = input_ids\n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            seq_len = generated_ids.size(1)\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=generated_ids.device), diagonal=1).bool()\n",
    "            causal_mask = causal_mask.float().masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "            outputs = self.forward(generated_ids)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "            next_token_ids = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n",
    "            \n",
    "            # Check for eos_token_id in the batch\n",
    "            if (next_token_ids == tokenizer.eos_token_id).any():\n",
    "                break\n",
    "        return generated_ids\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "embed_dim = 1144\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "hidden_dim = 2288\n",
    "dropout = 0.1\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "batch_size = 8\n",
    "epochs = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss(logits, hard_labels, soft_logits, alpha=0.5, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    soft_logits = soft_logits / temperature\n",
    "\n",
    "    kl_loss = nn.functional.kl_div(\n",
    "        torch.log_softmax(logits, dim=-1), \n",
    "        torch.softmax(soft_logits, dim=-1), \n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "\n",
    "    ce_loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), hard_labels.view(-1))\n",
    "\n",
    "    return alpha * kl_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_references = []\n",
    "    all_candidates = []\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i % 10 == 0: \n",
    "                print(i)\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            generated_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False, early_stopping = True, num_beams = 3, attention_mask = attention_mask)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "            if logits.size(1) != labels.size(1):\n",
    "                logits = logits[:, :labels.size(1), :]\n",
    "\n",
    "            for j in range(labels.size(0)):\n",
    "                input_ids_j = input_ids[j]\n",
    "                label_ids = labels[j]\n",
    "                pred_ids = generated_ids[j]\n",
    "\n",
    "                input_ids_text = tokenizer.decode(\n",
    "                    [id for id in input_ids_j if 0 <= id < tokenizer.vocab_size and id not in tokenizer.all_special_ids], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                label_text = tokenizer.decode(\n",
    "                    [id for id in label_ids if 0 <= id < tokenizer.vocab_size and id not in tokenizer.all_special_ids], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                pred_text = tokenizer.decode(\n",
    "                    [id for id in pred_ids if 0 <= id < tokenizer.vocab_size and id not in tokenizer.all_special_ids], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                print(\"pred:\")\n",
    "                print(pred_text[len(input_ids_text):])\n",
    "                print(\"end pred:\")\n",
    "\n",
    "                all_references.append(label_text.split())\n",
    "                all_candidates.append(pred_text.split())\n",
    "\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(all_references, all_candidates, smoothing_function=smoothing_function)\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "def load_model(model_class, tokenizer_class, model_path, config_class, pretrained_model_name):\n",
    "    config = config_class.from_pretrained(model_path)\n",
    "    model = model_class.from_pretrained(model_path, config=config)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def train_student(student, dataloader, teacher_model, model_name, epochs=2, alpha=0.5, temperature=1.0, learning_rate=5e-5, weight_decay=0.01):\n",
    "    student.train()\n",
    "    student.to(device)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    optimizer = AdamW(student.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        epoch_iterator = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch')\n",
    "        for i, batch in enumerate(epoch_iterator):\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = student(input_ids)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(input_ids, attention_mask).logits\n",
    "                    hard_labels = teacher_logits.view(-1, teacher_logits.size(-1)).argmax(dim=-1).to(device)\n",
    "\n",
    "                total_batch_loss = combined_loss(logits, hard_labels, teacher_logits, alpha, temperature)\n",
    "            \n",
    "            scaler.scale(total_batch_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += total_batch_loss.item()\n",
    "\n",
    "            epoch_iterator.set_postfix(loss=total_loss / (i + 1))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    torch.save(student.state_dict(), f\"{model_name}.pt\")\n",
    "    print(f\"Model saved as {model_name}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = DecoderOnlyModel(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, dropout).to(device)\n",
    "#teacher_model = GPT2LMHeadModel.from_pretrained(\"./teacher_model_fine_tuned\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_student(student_model, dataloader, model, model_name=\"student model\" , epochs=5, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(model, test_dataloader, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Comment: Me too. He might have gotten away, but it wouldn't be with my cat or all his body parts. I would have scratched his eyes out and pummeled his testicles into mush!\"\n",
    "definition = 'Task: Classify the following comment as sexual-explicit or non-sexual-explicit. A comment is considered sexual-explicit if it explicitly portrays sexual matters. give me only the classafaction, without an explanation.'\n",
    "system = 'System: This is a classification task. You need to classify the comment based on its content.'\n",
    "\n",
    "# Combine the components into a prompt\n",
    "prompt = f\"{definition}\\n{input_text}\"\n",
    "\n",
    "# Encode the prompt with padding\n",
    "encoding = tokenizer(prompt, return_tensors='pt', padding=False).to(device)\n",
    "\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Ensure inputs are in float32\n",
    "input_ids = input_ids\n",
    "\n",
    "# Function to stop at first newline\n",
    "def generate_until_newline(model, tokenizer, input_ids, max_new_tokens=50):\n",
    "    generated_text = \"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model.generate(\n",
    "            input_ids.to(torch.int64),  # Convert back to int64 for generation\n",
    "            max_new_tokens=1,  # Generate one token at a time\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,  # Deterministic generation\n",
    "            temperature=1,  # Set temperature to 0 for greedy decoding\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # Ensure padding is handled correctly\n",
    "        )\n",
    "        new_token = outputs[:, -1:]\n",
    "        new_token_text = tokenizer.decode(new_token[0], skip_special_tokens=True)\n",
    "        \n",
    "\n",
    "        generated_text += new_token_text\n",
    "        input_ids = torch.cat([input_ids, new_token.float()], dim=-1)\n",
    "    return generated_text\n",
    "\n",
    "# Generate the output\n",
    "generated_text = generate_until_newline(model, tokenizer, input_ids)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
